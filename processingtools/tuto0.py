from multipro7 import *
from labex.tools.generic_nobspy import primes 
import time


"""
parallelize an operation,
understand the general structure of the code
"""

def TargetFunction(t):
    #the function to be paralellized

    #this function will be attached to each worker 
    #they get jobs from an input buffer (queue) and dispose the result on another one
    #if the input queue is empty, the worker will wait until a job is generated by the jobgenerator (see below)
    #if the output queue is full, the worker will wait until one slot is freed

    #in this example, we compute prime numbers until time (t) is up and return the last one

    start = time.time()
    pgen = primes(1e6) #prime generator
    while time.time() - start < t:
        p = pgen.next()
    pgen.close()
    return t, p

def JobGenerator(N):
    #the job generator

    #this function will run into a separate workspace
    #the generated jobs will be disposed in the input queue 
    #as sone as a worker is available, it will get one job from it.
    #The job generator will try to feed the input queue continuously

    for jobid in xrange(N):
        t = np.random.rand() * 2.#pick a random time
        yield Job(t) #pass the arguments of the target function to a Job
                     #use yield instead of return!!!!



#the mapper will create the workers, and attach the target function to them
#it is itself an iterator object (that can be explored using for loop), which will return the results from the workers
with MapAsync(TargetFunction, JobGenerator(100)) as ma:
    print ma #print the process id numbers of each independent process (the job generator has is own pid)
    for jobid, answer, _, _ in ma: #each ouput has 4 items, the two firsts are most important
        t, p = answer #get the results from the jobid'th job here
        ma.communicate("job %6d returned %f %d" % (jobid, t, p)) #avoid using print inside the with statement, this could mess up with verbose mode (see next tutorials)

#note that
# the jobs are not returned in the input order since they use different computation time, use jobid to re-arange the jobs in the correct order
# use MapSync instead of MapAsync to force the jobs to be returned in the right order (the jobs will be stored until previous jobs have been returned)
# please remember that the workers will stop if the output queue is full. Then, the operations enclosed inside the "with" statment must be faster than
# the processing time to ensure the CPUs to work at full regime.
# have a look at the memory and CPU usage during processing using htop


#this structure can be used for almost any parallelization problem

#WARNING :
#do not use numpy.random inside the TargetFunction, see next tutorials if you need to use random numbers inside the target function
#(reason : numpy.random is not thread safe, random numbers are generated based on the internal clock, so they could be similar between different processes, which may bias random series)



    










